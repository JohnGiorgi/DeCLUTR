{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8jt6ML03DS5"
   },
   "source": [
    "# Training your own model\n",
    "\n",
    "This notebook will walk you through training your own model using [DeCLUTR](https://github.com/JohnGiorgi/DeCLUTR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SU3Iod2-g0-o"
   },
   "source": [
    "## üîß Install the prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sr4r5pN40Kli"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/JohnGiorgi/DeCLUTR.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zog7ApwuUD7_"
   },
   "source": [
    "## üìñ Preparing a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwnLpUmN4Art"
   },
   "source": [
    "\n",
    "A dataset is simply a file containing one item of text (a document, a scientific paper, etc.) per line. For demonstration purposes, we have provided a script that will download the [WikiText-103](https://www.salesforce.com/products/einstein/ai-research/the-wikitext-dependency-language-modeling-dataset/) dataset and format it for training with our method.\n",
    "\n",
    "The only \"gotcha\" is that each piece of text needs to be long enough so that we can sample spans from it. In general, you should collect documents of a minimum length according to the following:\n",
    "\n",
    "```python\n",
    "min_length = num_anchors * max_span_len * 2\n",
    "```\n",
    "\n",
    "In our paper, we set `num_anchors=2` and `max_span_len=512`, so we require documents of `min_length=2048`. We simply need to provide this value as an argument when running the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0fwnwq23aAZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_data_path = \"wikitext_103/train.txt\"\n",
    "min_length = 2048\n",
    "\n",
    "!wget -nc https://raw.githubusercontent.com/JohnGiorgi/DeCLUTR/master/scripts/preprocess_wikitext_103.py\n",
    "!python preprocess_wikitext_103.py $train_data_path --min-length $min_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yUEFeupP6qy-"
   },
   "source": [
    "Lets confirm that our dataset looks as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7ffGXCn7Cpq"
   },
   "outputs": [],
   "source": [
    "!wc -l $train_data_path  # This should be approximately 17.8K lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "10DprWZc9iV6"
   },
   "outputs": [],
   "source": [
    "!head -n 1 $train_data_path  # This should be a single Wikipedia entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VKYdambZ59nM"
   },
   "source": [
    "## üèÉ Training the model\n",
    "\n",
    "Once you have collected the dataset, you can easily initiate a training session with the `allennlp train` command. An experiment is configured using a [Jsonnet](https://jsonnet.org/) config file. Lets take a look at the config for the DeCLUTR-small model presented in [our paper](https://arxiv.org/abs/2006.03659):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xTaSExh4ba8e"
   },
   "outputs": [],
   "source": [
    "!wget -nc https://raw.githubusercontent.com/JohnGiorgi/DeCLUTR/master/training_config/declutr_small.jsonnet\n",
    "with open(\"declutr_small.jsonnet\", \"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-f1HqWSscWOx"
   },
   "source": [
    "\n",
    "The only thing to configure is the path to the training set (`train_data_path`), which can be passed to `allennlp train` via the `--overrides` argument (but you can also provide it in your config file directly, if you prefer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YS9VuxESBcr3"
   },
   "outputs": [],
   "source": [
    "overrides = (\n",
    "    f\"{{'train_data_path': '{train_data_path}', \"\n",
    "    # lower the batch size to be able to train on Colab GPUs\n",
    "    f\"'data_loader.batch_size': 2, \"\n",
    "    # training examples / batch size. Not required, but gives us a more informative progress bar during training\n",
    "    f\"'data_loader.batches_per_epoch': 8912}}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Db_cNfZ76KRf"
   },
   "outputs": [],
   "source": [
    "!allennlp train \"declutr_small.jsonnet\" \\\n",
    "    --serialization-dir \"output\" \\\n",
    "    --overrides \"$overrides\" \\\n",
    "    --include-package \"declutr\" \\\n",
    "    -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting a trained model to HuggingFace Transformers\n",
    "\n",
    "We have provided a simple script to export a trained model so that it can be loaded with [Hugging Face Transformers](https://github.com/huggingface/transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://github.com/JohnGiorgi/DeCLUTR/blob/master/scripts/save_pretrained_hf.py\n",
    "!python save_pretrained_hf.py --archive-file \"output\" --save-directory \"output_transformers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model, saved to `--save-directory`, can then be loaded using the Hugging Face Transformers library\n",
    "\n",
    "> See the [embedding notebook](https://colab.research.google.com/github/JohnGiorgi/DeCLUTR/blob/master/notebooks/embedding.ipynb) for more details on using trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "  \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"output_transformers\")\n",
    "model = AutoModel.from_pretrained(\"output_transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eD5dZo18EE-S"
   },
   "source": [
    "## ‚ôªÔ∏è Conclusion\n",
    "\n",
    "That's it! In this notebook, we covered how to collect data for training the model, and specifically how _long_ that text needs to be. We then briefly covered configuring and running a training session. Please see [our paper](https://arxiv.org/abs/2006.03659) and [repo](https://github.com/JohnGiorgi/DeCLUTR) for more details, and don't hesitate to open an issue if you have any trouble!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "training.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
